#!/usr/bin/env python




from __future__ import absolute_import, division, print_function

from builtins import (bytes, str, open, super, range,
                      zip, round, input, int, pow, object, map, zip)

__author__ = "Andrea Tramacere"

# Standard library
# eg copy
# absolute import rg:from copy import deepcopy

# Dependencies
# eg numpy
# absolute import eg: import numpy as np
import  argparse
import copy
# Project
# relative import eg: from .mod import f
from sklearn.base import clone


from PrimalCore.model_selection.model_validation import   model_learning_curve,model_validation_curve
from PrimalCore.heterogeneous_table.table import    Table
from PrimalCore.heterogeneous_table.tools import build_names_list
from PrimalCore.homogeneous_table.dataset_handler    import *
from PrimalInteractive.plotting.phz_plots import  *

from PrimalCore.phz_tools.photometry import FluxRatio
from PrimalCore.models.regression import Regressor
from sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor,AdaBoostRegressor
from PrimalCore.models.base import BaseModel
from PrimalCore.preprocessing.dataset_preprocessing import *
from PrimalCore.phz_tools.stats import outliers_score
from PrimalCore.models.pipeline import pipeline_stratified_kfold_cv
from PrimalCore.pdf.stats import eval_pit,eval_pdf_gmm,eval_crps
from PrimalCore.pdf.tools import extract_pdf
import numpy as np


def load_preproc_catalog(ph_catalog,target_col_name,cat_col_name_reg_ex=None,fits_file_ext=0,skip_cleaning=False):
    """
    Catalog loading

    Parameters
    ----------
    ph_catalog
    target_col_name
    cat_col_name_reg_ex
    fits_file_ext

    Returns
    -------

    """
    catalog=Table.from_fits_file(ph_catalog,fits_ext=fits_file_ext)

    if cat_col_name_reg_ex is not None:
        catalog.keep_columns(cat_col_name_reg_ex,regex=True)

    if skip_cleaning is False:
        CLEAN= (catalog.data['FLAG_PHOT']==0)*(catalog.data['MASKED']==0)*(catalog.data['STAR']==0)
        CLEAN*=(catalog.data['AGN']==0)*(catalog.data['reliable_S15']==1)
        catalog.keep_rows(CLEAN)


    dataset=MLDataSet.new_from_table(catalog,target_col_name=target_col_name,target_bins=20,target_binning='log',catalog_file=ph_catalog)
    return dataset


def add_flux_ratio(dataset,overwrite=True):
    flux_bands_list_2 = ['FLUX_G_2', 'FLUX_R_2', 'FLUX_I_2', 'FLUX_Z_2', 'FLUX_Y_2', 'FLUX_J_2', 'FLUX_VIS', 'FLUX_VIS',
                         'FLUX_VIS']
    flux_bands_list_1 = ['FLUX_R_2', 'FLUX_I_2', 'FLUX_Z_2', 'FLUX_Y_2', 'FLUX_J_2', 'FLUX_H_2', 'FLUX_Y_2', 'FLUX_J_2',
                         'FLUX_H_2']

    # This was input cell with execution count: 37
    if overwrite is True:
        _l=[]
        for f1, f2 in zip(flux_bands_list_1, flux_bands_list_2):
            f1_name = f1.split('_')[1]
            f2_name = f2.split('_')[1]
            if f1 in dataset.features_names and f2 in dataset.features_names:
                _l.append('F_RATIO_%s' % (f2_name + '-' + f1_name))
        drop_features(dataset,_l)


    for f1, f2 in zip(flux_bands_list_1, flux_bands_list_2):
        f1_name = f1.split('_')[1]
        f2_name = f2.split('_')[1]
        if f1 in dataset.features_names and f2 in dataset.features_names:
            f = FluxRatio('F_RATIO_%s' % (f2_name + '-' + f1_name), f1, f2, features=dataset)
            if overwrite is True and f.name in dataset.features_names:
                dataset.set_feature(f.name,f.values)
            else:
                add_features(dataset, f.name, f.values)


def preproc_mldataset(dataset,feat_col_name_reg_ex=None,train_ratio=0.1,stratify=True,split=True,error_col_name_reg_ex=None):
    """
    Preprocessing of the dataset with evaluation of flux ratios

    Parameters
    ----------
    dataset
    feat_col_name_reg_ex
    train_ratio
    stratify

    Returns
    -------

    """

    add_flux_ratio(dataset)
    #print(dataset.features_names)
    err_names=None
    #print('1',build_names_list(feat_col_name_reg_ex,dataset.features_names))
    #print('2',build_names_list(error_col_name_reg_ex,dataset.features_names))

    #if error_col_name_reg_ex is not None:
    #    add_list_reg_ex=build_names_list(feat_col_name_reg_ex,dataset.features_names).extend(error_col_name_reg_ex)[:]
    #    err_names=build_names_list(error_col_name_reg_ex, dataset.features_names)
    #else:
    #    add_list_reg_ex=add_list_reg_ex[:]

    add_list_reg_ex=None
    if feat_col_name_reg_ex is not None:
        add_list_reg_ex=feat_col_name_reg_ex[:]

        if error_col_name_reg_ex is not None:
            if type(error_col_name_reg_ex)==list:
                add_list_reg_ex.extend(error_col_name_reg_ex)
            else:
                add_list_reg_ex.append(error_col_name_reg_ex)

    print('add_list_reg_ex',add_list_reg_ex)

    if add_list_reg_ex is not None:
        keep_features(dataset, add_list_reg_ex , regex=True)

    if err_names is not None:
        for err_name in err_names:
            for (ID, f_name) in enumerate(dataset.features_names):
                if err_name == f_name:
                    dataset.columns_mask[ID] = False

    if split==True:
        train,test=dataset_train_test_split(dataset,train_ratio=train_ratio,stratify=stratify)

    else:
        train=None
        test=dataset

    return  train,test

def randomize_test(dataset,error_col_name_reg_ex,feat_col_name_reg_ex,num_random_samples):
    random_set=np.zeros((num_random_samples,dataset.features_N_rows,dataset.columns_mask.sum()))

    _d=copy.deepcopy(dataset)
    build_names_list(error_col_name_reg_ex, dataset.features_names)
    err_names_list = build_names_list(error_col_name_reg_ex, dataset.features_names)
    feat_name_list=build_names_list(feat_col_name_reg_ex,dataset.features_names)
    print(dataset.features_names)
    for ID in range(num_random_samples):
        print('random repl',ID)
        for (e,f) in zip(err_names_list,feat_name_list):
            _d.set_feature(f,np.random.normal(dataset.get_feature_by_name(f),dataset.get_feature_by_name(e)))
        add_flux_ratio(_d)
        random_set[ID]=_d.features

    return  random_set

def build_models(use_models_list=None):
    """
    builds a list of models

    Parameters
    ----------
    use_models_list

    Returns
    -------

    """
    models_list=[]
    models_list.append(Regressor.ABRegressor(n_estimators=100,name='ada_boost'))
    models_list.append(Regressor.KNRegressor(name='KN'))
    models_list.append(Regressor.RFRegressor(n_estimators=100,name='random_forest'))
    models_list.append(Regressor.GBRegressor(n_estimators=100,name='gradient_boosting'))
    if use_models_list is not None:
        for model in models_list[:]:
            if model.name not in use_models_list:
                models_list.remove(model)

    print (models_list)
    return models_list



def feature_selection_results(model,train,n_features, scores,plot=False,saveplot=False,flag=None):
    """
    Evaluate the features importance for the model, (makes/saves plots)

    Parameters
    ----------
    model
    train
    n_features
    scores
    plot
    saveplot
    flag

    Returns
    -------

    """
    _name=model.name
    if flag is not None:
        _name = '%s' % flag + '_' + _name

    print("selected features", n_features, np.array(train.features_names))
    feat_imp = model.feature_importances(feature_names=np.array(train.features_names))
    if feat_imp is not None:
        if saveplot == True or plot == True:
            p = plot_features_imp(feat_imp['score'], feature_names=feat_imp['name'], plot=plot)
            p1 = plot_feature_imp_trend(scores,plot=plot)
        if saveplot==True:
            p.save_plot('%s_feat_imp_post_selection.pdf' % (_name))
            p1.save_plot('%s_feat_imp_post_selection_trend.pdf' % (_name))


def single_regression(model,train,test,saveplot=False,plot=False,title='single_regression',fit=True):
    """
    Test set results for a single regression  (makes/saves plots)

    Parameters
    ----------
    model
    train
    test
    saveplot
    plot
    title

    Returns
    -------

    """
    if fit==True:
        model.clf.fit(train.features, train.target_array)

    z_spec = test.target_array
    z_phot = model.clf.predict(test.features)

    if saveplot == True or plot==True:
        p = plot_z_spec_vs_z_phot(z_spec, z_phot, plot=plot)

    if saveplot == True:
        p.save_plot('%s.pdf'%(title))


def pdf(model,
        test,
        pdf_grid_size,
        pdf_grid_min,
        pdf_grid_max,
        test_random_array=None,
        saveplot=False,
        plot=False,
        skip_gmm=False,
        flag=None,
        out_flag=None):

    print('start pdf')
    _name = model.name
    if out_flag is not None:
        _name = '%s' % out_flag + '_' + _name

    pdf_array = extract_pdf(model,
                            test,
                            randomized_datasets=test_random_array,
                            pdf_grid_size=pdf_grid_size,
                            pdf_grid_min=pdf_grid_min,
                            pdf_grid_max=pdf_grid_max,
                            out_file_name='%s_pdf_%s.fits' % (_name,flag),skip_gmm=skip_gmm)
    pit = eval_pit(pdf_array['z_phot_values'], pdf_array['z_spec'])
    pit_plot = plot_PIT_histogram(pit, plot=plot)



    if saveplot == True:
        pit_plot.save_plot('%s_PIT_%s.pdf' % (_name,flag))

    cprs = eval_crps(pdf_array['z_phot_values'],pdf_array['z_spec'])
    cprs_plot = plot_CRPS_histogram(cprs, plot=plot)
    if saveplot == True:
        cprs_plot.save_plot('%s_CPRS_%s.pdf' % (_name,flag))
    print('pdf done')



def model_validation(model,train,saveplot=False,plot=False,scoring=None,flag=None):
    """
    Sklearn based model validation

    Parameters
    ----------
    model
    train
    saveplot
    plot
    scoring
    flag

    Returns
    -------

    """
    train_sizes, train_scores, test_scores=model_learning_curve(model,train,scoring=scoring)

    _name = model.name
    if flag is not None:
        _name = '%s' % flag + '_' + _name

    if model.name in ['ada_boost', 'random_forest', 'gradient_boosting']:

        param_name = 'n_estimators'
        param_range = np.linspace(1, 500, 5).astype(np.int32)

    elif model.name == 'SVR':
        param_name = 'gamma'
        param_range = np.logspace(-6, -1, 10)

    elif model.name == 'KNeighborsRegressor':
        param_name = 'n_neighbors'
        param_range = np.linspace(1, 10, 5).astype(np.int32)

    else:
        raise RuntimeError("model %s  not supported" % model.name)

    if saveplot == True or plot == True:
        p_lc =plot_learning_curve(train_sizes, train_scores, test_scores,plot=plot)


    if saveplot == True:
        p_lc.save_plot('%s_learning_curve.pdf' % (_name))


    train_scores, test_scores=model_validation_curve(model,param_range,param_name,train,scoring=scoring)


    if saveplot == True or plot == True:
        p_vc = plot_validation_curve(train_scores, test_scores,param_range,param_name,'outliers', plot=plot)

    if saveplot == True:
        p_vc.save_plot('%s_validation_curve.pdf' % (_name))


def predict_trained(test,
                    model,
                    pdf_grid_size,
                    pdf_grid_min,
                    pdf_grid_max,
                    test_random_array=None,
                    plot=False,
                    out_flag=None,
                    saveplot=False,
                    skip_gmm=False):

    _name = model.name

    if out_flag is not None:
        _name = '%s' % out_flag + '_' + _name


    single_regression(model, None, test, saveplot=saveplot, plot=plot,
                      title='%s_single_regression_from_saved' % _name,fit=False)
    model.save_mldataset_predictions('%s_single_regression_results_from_saved.fits' % (_name), test)
    pdf(model,
        test,
        pdf_grid_size,
        pdf_grid_min,
        pdf_grid_max,
        test_random_array=test_random_array,
        flag='post_opt',
        saveplot=saveplot,
        skip_gmm=skip_gmm,
        out_flag=out_flag)


def train_and_predict(model_list,
                      train,
                      test,
                      pdf_grid_size,
                      pdf_grid_min,
                      pdf_grid_max,
                      test_random_array=None,
                      plot=False,
                      saveplot=False,
                      out_flag=None,
                      skip_gmm=False,
                      savemodel=False):

    if test_random_array is not None:
        print('test_random_array',test_random_array.shape)
    for model in model_list:
        _name=model.name

        if out_flag is not None:
            _name='%s'%out_flag+'_'+_name

        p = pipeline_stratified_kfold_cv(model, scorer=outliers_score)

        # one shot no pipeline
        single_regression(p.model, train, test, saveplot=saveplot, plot=plot,
                          title='%s_single_regression_test_results_pre_opt' %_name)

        model.save_mldataset_predictions('%s_single_regression_test_results_pre_opt.fits' % (_name), test)

        #pdf(p.model,
        #    test,
        #    pdf_grid_size,
        #    pdf_grid_min,
        #    pdf_grid_max,
        #    test_random_array=test_random_array,
        #    flag='pre_opt',
        #    saveplot=saveplot,
        #    skip_gmm=skip_gmm,
        #    out_flag=out_flag)


        #This run the full pipeline
        print('--> preselection features',train.features.shape)
        p.fit(train.features,train.target_array)


        #gets results form rfecv in pipeline
        support=p.sk_pipeline.named_steps['rfecv'].support_
        n_features=p.sk_pipeline.named_steps['rfecv'].n_features_
        scores=p.sk_pipeline.named_steps['rfecv'].grid_scores_

        train.selection_mask=support
        test.selection_mask=support

        #plot rfecv results
        print('--> postselection features',train.features.shape)
        feature_selection_results(p.model,train,n_features, scores,plot=plot,saveplot=saveplot,flag=out_flag)

        #eval model validation for the trained pipeline model
        model_validation(p.model,train,saveplot=saveplot,scoring=outliers_score,flag=out_flag)

        #gets final performance for trained pipeline model

        clf = clone(model.clf)
        model.clf=clf
        single_regression(p.model, train, test, saveplot=saveplot, plot=plot, title='%s_single_regression_test_results_post_opt' % _name)

        pdf(p.model,
            test,
            pdf_grid_size,
            pdf_grid_min,
            pdf_grid_max,
            test_random_array=test_random_array,
            flag='post_opt',
            saveplot=saveplot,
            skip_gmm=skip_gmm,
            out_flag=out_flag)

        model.save_mldataset_predictions('%s_single_regression_test_results_post_opt.fits' % (_name), test)
        if savemodel==True:

           model.save(file_name=model.name+'_trained.pkl')


def run(input_fits_file,
        fits_file_ext,
        target_column_name,
        cat_col_name_reg_ex,
        feat_col_name_reg_ex,
        pdf_grid_size,
        pdf_grid_min,
        pdf_grid_max,
        use_models_list=[],
        train_test_split_ratio=0.1,
        error_col_name_reg_ex=None,
        num_random_samples=500,
        plot=False,
        saveplot=False,
        out_flag=None,
        skip_gmm=False,
        savemodel=False,
        loadmodel=False,
        skip_cleaning=False):


    if loadmodel is not None:
        split=False
        model = BaseModel.load(loadmodel)
    else:
        split=True
        model_list = build_models(use_models_list)

    ml_dataset = load_preproc_catalog(input_fits_file,
                                      target_col_name=target_column_name,
                                      cat_col_name_reg_ex=cat_col_name_reg_ex,
                                      fits_file_ext=fits_file_ext,
                                      skip_cleaning=skip_cleaning)


    train, test = preproc_mldataset(ml_dataset,
                                    train_ratio=train_test_split_ratio,
                                    feat_col_name_reg_ex=feat_col_name_reg_ex,
                                    error_col_name_reg_ex=error_col_name_reg_ex,
                                    split=split)

    test_random_array=None
    if error_col_name_reg_ex is not None and num_random_samples is not None:
        test_random_array=randomize_test(test,error_col_name_reg_ex,feat_col_name_reg_ex,num_random_samples)

    if loadmodel is None:
        train_and_predict(model_list,
                          train,
                          test,
                          pdf_grid_size,
                          pdf_grid_min,
                          pdf_grid_max,
                          test_random_array,
                          plot=plot,
                          saveplot=saveplot,
                          out_flag=out_flag,
                          skip_gmm=skip_gmm,
                          savemodel=savemodel)
    else:
        predict_trained(test,model,
                        pdf_grid_size,
                        pdf_grid_min,
                        pdf_grid_max,
                        test_random_array,
                        saveplot=saveplot,
                        skip_gmm=skip_gmm,
                        plot=plot)



def main(argv=None):
    parser = argparse.ArgumentParser()


    parser.add_argument('input_fits_file', type=str, default=None )
    parser.add_argument('fits_file_ext', type=int, default=0 )
    parser.add_argument('target_column_name', type=str, default='z_spec_S15' )
    parser.add_argument('-cat_col_name_reg_ex', type=str, nargs='+',default= ['FLUX*2','FLUX_VIS','FLUXERR*2','FLUXERR*VIS','reliable_S15','STAR','AGN','MASKED','FLAG_PHOT','z_spec_S15'])
    parser.add_argument('-feat_col_name_reg_ex', type=str, nargs='+',default= ['FLUX*2','FLUX*VIS','F_RATIO*'])
    parser.add_argument('-error_col_name_reg_ex', type=str, nargs='+', default=['FLUXERR*2','FLUXERR*VIS'])
    parser.add_argument('-use_models_list', type=str, default=['ada_boost'],nargs='+')
    parser.add_argument('-num_random_samples', type=int, default=None)
    parser.add_argument('-train_test_split_ratio',type=np.float,default=0.1)
    parser.add_argument('-skip_gmm', action='store_true')
    parser.add_argument('-plot',action='store_true')
    parser.add_argument('-save_model', action='store_true')
    parser.add_argument('-load_model', type=str, default=None)
    parser.add_argument('-saveplot',action='store_true')
    parser.add_argument('-skip_cleaning', action='store_true')
    parser.add_argument('-out_flag',type=str, default=None)
    parser.add_argument('-pdf_grid_size', type=np.int, default=100)
    parser.add_argument('-pdf_grid_min', type=np.float, default=None)
    parser.add_argument('-pdf_grid_max', type=np.float, default=None)

    args = parser.parse_args()

    print(args)

    run(args.input_fits_file,
        args.fits_file_ext,
        args.target_column_name,
        args.cat_col_name_reg_ex,
        args.feat_col_name_reg_ex,
        args.pdf_grid_size,
        args.pdf_grid_min,
        args.pdf_grid_max,
        error_col_name_reg_ex=args.error_col_name_reg_ex,
        num_random_samples=args.num_random_samples,
        use_models_list=args.use_models_list,
        train_test_split_ratio=args.train_test_split_ratio,
        plot=args.plot,
        saveplot=args.saveplot,
        out_flag=args.out_flag,
        skip_gmm=args.skip_gmm,
        savemodel=args.save_model,
        loadmodel=args.load_model,
        skip_cleaning=args.skip_cleaning)



if  __name__ == "__main__":
    main()
